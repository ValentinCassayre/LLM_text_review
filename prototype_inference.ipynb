{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a64e7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee03da5e",
   "metadata": {},
   "source": [
    "# Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb77127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5a6e893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out the next word in this sentence: \"I am the last US president, my name is __.\" Let's break this down step by step.\n",
      "\n",
      "First, let me look at the beginning of the sentence: \"I am the last US president.\" The speaker is talking about their current status as a U.S. President. They've just become the last one in the presidency, which is pretty cool because that means they're done with their term. So, the key here is to continue that phrase.\n",
      "\n",
      "Now, after \"president,\" we can think of other words related to the role of president or possibly moving forward into the next chapter. The common word that comes to mind immediately is \"life.\" Because being a president typically involves serving long-term, and eventually, one might consider handing over the presidency, so perhaps they're about to finish their life as a president.\n",
      "\n",
      "Let me check if there are other possible words. Words like \"legacy\" come to mind because it refers to the lasting impact or succession of someone's work. That could also fit, but \"life\" feels more direct and likely expected here since it's a common future continuation of that role.\n",
      "\n",
      "To make sure I'm not missing something, let me think about other possibilities. There's \" legacy,\" which is associated with the ongoing effects of one's actions, but in this context, it might not be as clear. Similarly, \"terminal\" could imply the end or completion, but that's more specific and perhaps too strong.\n",
      "\n",
      "\"Legacy\" might work, but I'm inclined towards \"life.\" I think in English, when someone refers to their current role as a president being the last one, they're probably looking for something that signifies their departure. So, \"life\" makes sense because it conveys the completion of their term and their readiness to move on.\n",
      "\n",
      "Another angle is considering if there's an acronym or specific term used in history or politics when referring to the end of a presidency. I'm not sure about any standard terms like \"retired\" or \"卸任,\" but those are less common and might be worded differently. So, \"life\" seems more likely.\n",
      "\n",
      "I should also think about the structure of the sentence. The first part is a sentence that ends with \"president.\" After that, we usually continue the sentence by adding words related to their current status or future actions. Since they're finishing their term, it's logical to include words like \"life\" or \"terminal.\"\n",
      "\n",
      "Considering all this, I think the most fitting word after \"president\" is \"life,\" which signifies the completion of serving as a U.S. President.\n",
      "</think>\n",
      "\n",
      "The next word in the sentence should be \"life.\" This fits because being the last president involves finishing their term, and \"life\" conveys the completion of their role.\n"
     ]
    }
   ],
   "source": [
    "model_name_ollama = 'deepseek-r1:1.5b'\n",
    "context = 'You have to guess the next word of the following text written by a human, please just anwser the missing word :'\n",
    "text = 'I am the last US president, my name is'\n",
    "prompt = f'{context}\\n\\n{text}'\n",
    "\n",
    "response = ollama.chat(model=model_name_ollama, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "print(response.get('message', {}).get('content', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1053463c",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d891f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb45183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6fe7e",
   "metadata": {},
   "source": [
    "You are a text reviewing tool utilizing a LLM to correct bad tokens in a text. We input you the beginning of the text and you will only compute the logits of the next token. We then represent the logit given to the actual word for the reviewer. The logit is converted to a probability using softmax and is represented with a color in logarithmic scale. The user can then understand if it made a mistake if the probability is too low. Make extra caution to avoid predicting any factual or grammatical errors by giving them high logits. You have no choice but to predict correct tokens, and you cannot assign high logits to wrong tokens.\n",
    "\n",
    "Exemple of input prompt :\n",
    "Water boils at 100°C at standard atmospheric\n",
    "\n",
    "You will try to predict the next token, a good prediction would be \"pressure\". If the actual token written by the user was \"pressure\" the user would see that the prediction is good. Otherwise, if the user writes something bad, for instance \"temperature\", which doesn't make sense, the logit of this token should be low and the user would see its mistake.\n",
    "\n",
    "Exemple of input prompt :\n",
    "I love eleph\n",
    "\n",
    "You will try to predict the next token, a good prediction would be \"ants\". If the actual token written by the user was \"ants\" the user would see that the prediction is good. Otherwise, if the user writes something bad, for instance \"ent\", which is a spelling mistake, the logit of this token should be low and the user would see its mistake.\n",
    "\n",
    "This is the input prompt :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "919e44ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%, current_token : '<｜begin▁of▁sentence｜>', next_token : 'Football', prob : 0.0000, rank : 129442, predicted :\n",
      "' \"'\n",
      "' \\n\\n'\n",
      "' \\n'\n",
      "' This'\n",
      "' ['\n",
      "20.0%, current_token : 'Football', next_token : ' is', prob : 0.2653, rank : 1, predicted :\n",
      "' is'\n",
      "' players'\n",
      "' teams'\n",
      "','\n",
      "'\\n'\n",
      "40.0%, current_token : ' is', next_token : ' a', prob : 0.3835, rank : 1, predicted :\n",
      "' a'\n",
      "' the'\n",
      "' an'\n",
      "' one'\n",
      "' being'\n",
      "60.0%, current_token : ' a', next_token : ' fruit', prob : 0.0000, rank : 895, predicted :\n",
      "' game'\n",
      "' sport'\n",
      "' very'\n",
      "' team'\n",
      "' popular'\n"
     ]
    }
   ],
   "source": [
    "text = \"Football is a fruit\"\n",
    "\n",
    "base_prompt = \"\"\"You are a text reviewing tool utilizing a LLM to correct bad tokens in a text. We input you the beginning of the text and you will only compute the logits of the next token. We then represent the logit given to the actual word for the reviewer. The logit is converted to a probability using softmax and is represented with a color in logarithmic scale. The user can then understand if it made a mistake if the probability is too low. Make extra caution to avoid predicting any factual or grammatical errors by giving them high logits. You have no choice but to predict correct tokens, and you cannot assign high logits to wrong tokens.\n",
    "\n",
    "Exemple of input prompt :\n",
    "Water boils at 100°C at standard atmospheric\n",
    "\n",
    "You will try to predict the next token, a good prediction would be \"pressure\". If the actual token written by the user was \"pressure\" the user would see that the prediction is good. Otherwise, if the user writes something bad, for instance \"temperature\", which doesn't make sense, the logit of this token should be low and the user would see its mistake.\n",
    "\n",
    "Exemple of input prompt :\n",
    "I love eleph\n",
    "\n",
    "You will try to predict the next token, a good prediction would be \"ants\". If the actual token written by the user was \"ants\" the user would see that the prediction is good. Otherwise, if the user writes something bad, for instance \"ent\", which is a spelling mistake, the logit of this token should be low and the user would see its mistake.\n",
    "\n",
    "This is the input prompt :\"\"\"\n",
    "\n",
    "base_ids = tokenizer.encode(base_prompt, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "words = []\n",
    "probabilities = []\n",
    "preferreds = []\n",
    "ranks = []\n",
    "all_prob_pairs = []\n",
    "\n",
    "with torch.no_grad(): # inference\n",
    "    for i in range(len(input_ids[0]) - 1):\n",
    "        \n",
    "        # merge the pre-prompt (base prompt) with the actual prompt\n",
    "        # TODO fix base prompt often giving worse results\n",
    "        full_input_ids = torch.cat([base_ids, input_ids[:, :i + 1][:, 1:]], dim=-1) # input of the model : all of the text before the token it has to guess\n",
    "        #context = base_prompt_ids + input_tokens[:, :i + 1]\n",
    "        \n",
    "        # TODO maybe find better way instead of reinputing new context everytime\n",
    "        outputs = model(full_input_ids)\n",
    "        logits = outputs.logits[:, -1, :] # between -inf and +inf\n",
    "        probs = torch.softmax(logits, dim=-1) # between 0 and 1\n",
    "\n",
    "        vocab_size = probs.shape[-1]\n",
    "        token_ids = torch.arange(vocab_size)\n",
    "        tokens = [tokenizer.decode([token_id]) for token_id in token_ids] # TODO optimize?\n",
    "        token_probs = probs[0].tolist() # shape (1, vocab_size) to vocab_size\n",
    "        \n",
    "        token_prob_pairs = list(zip(tokens, token_probs))\n",
    "        sorted_token_prob_pairs = sorted(token_prob_pairs, key=lambda x: x[1], reverse=True)\n",
    "        dict_token_prob_pairs = dict(token_prob_pairs) # TODO useless if tried to be ranked, optimize?\n",
    "\n",
    "        current_token = tokenizer.decode([input_ids[0, i].item()])\n",
    "        \n",
    "        next_token = tokenizer.decode([input_ids[0, i+1].item()]) # next word (to be predicted)\n",
    "        prob = dict_token_prob_pairs[next_token] # probability of word predicted at best by LLM\n",
    "        pref = sorted_token_prob_pairs[0][0] # best predicted word by LLM\n",
    "        \n",
    "        # TODO optimize?\n",
    "        for index, (word, prob) in enumerate(sorted_token_prob_pairs):\n",
    "            if word == next_token:\n",
    "                rank = index + 1\n",
    "                break\n",
    "        \n",
    "        words.append(next_token)\n",
    "        probabilities.append(prob)\n",
    "        preferreds.append(pref)\n",
    "        all_prob_pairs.append(sorted_token_prob_pairs[:10])\n",
    "        ranks.append(rank)\n",
    "        \n",
    "        guess = [repr(e[0]) for e in sorted_token_prob_pairs[:5]]\n",
    "        print(f\"{i/len(input_ids[0])*100:.1f}%, current_token : {repr(current_token)}, next_token : {repr(next_token)}, prob : {prob:.4f}, rank : {rank}, predicted :\")\n",
    "        print(f\"{'\\n'.join(guess)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e0e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73446240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_cmap():\n",
    "    colors = [\n",
    "        (0.8, 0.1, 0.1),\n",
    "        (1.0, 1.0, 1.0),\n",
    "        (0.0, 0.5, 0.0),\n",
    "    ]\n",
    "    p_tran = 0.00001\n",
    "    positions = [0, p_tran, 1] # transition at p_tran\n",
    "    return LinearSegmentedColormap.from_list(\"custom_red_white_green\", list(zip(positions, colors)))\n",
    "\n",
    "def probability_to_color(prob, colormap):\n",
    "    rgba = colormap(prob)\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(int(rgba[0] * 255), int(rgba[1] * 255), int(rgba[2] * 255))\n",
    "\n",
    "def probability_to_color_plt(prob, colormap_name=\"Greens\"):\n",
    "    colormap = plt.get_cmap(colormap_name)\n",
    "    rgba = colormap(prob)\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(int(rgba[0] * 255), int(rgba[1] * 255), int(rgba[2] * 255))\n",
    "\n",
    "custom_cmap = create_custom_cmap()\n",
    "\n",
    "html = []\n",
    "html.append(\"<html><body style='padding: 20px; font-family: Arial; line-height: 2.0;'>\")\n",
    "html.append(\"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "  body { padding: 20px; font-family: Arial, sans-serif; line-height: 2.0; }\n",
    "  h1 { text-align: center; color: #333; font-size: 2.5em; margin-bottom: 20px; }\n",
    "  .word { background-color: #f0f0f0; padding: 2px 5px; border-radius: 3px; outline: 1px solid rgba(0, 0, 0, 0.2); font-weight: bold; }\n",
    "  h2 { color: #333; font-size: 1.8em; margin-bottom: 10px; }\n",
    "  .word { background-color: #f0f0f0; padding: 2px 5px; border-radius: 3px; outline: 1px solid rgba(0, 0, 0, 0.2); font-weight: bold; }\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "    \"\"\")\n",
    "html.append(\"<h1>Text review using LLMs</h1>\")\n",
    "html.append(\"<h2>Example text with no errors, factual statements</h2>\")\n",
    "html.append(\"<h2>Example text altered</h2>\")\n",
    "\n",
    "body = []\n",
    "for word, prob, pref, rank in zip(words, probabilities, preferreds, ranks):\n",
    "\n",
    "    cmap = 'Greens_r'\n",
    "    alpha = 100\n",
    "    if rank > 100:\n",
    "        cmap = 'Reds'\n",
    "        alpha = 200\n",
    "    #alpha = 255\n",
    "    #cmap = 'jet' # jet gives sharp insight but not nice to see\n",
    "    color = probability_to_color_plt(prob, cmap)\n",
    "    color = probability_to_color(prob, custom_cmap)\n",
    "    \n",
    "    colors = [(0.0, 0.5, 0.0),\n",
    "              (1.0, 1.0, 1.0),\n",
    "              (0.8, 0.1, 0.1)]\n",
    "\n",
    "    cmap = LinearSegmentedColormap.from_list('red_white_green', colors)\n",
    "    color = probability_to_color_plt(np.log10(prob)/np.log10(1e-8), cmap)\n",
    "\n",
    "    bonus = \"font-weight: bold;\" if pref == word else \"\"\n",
    "    bonus = ''\n",
    "    \n",
    "    style = f\"background-color: {color}{alpha:02x}; padding: 2px 5px; border-radius: 3px; outline: 1px solid rgba(0, 0, 0, .2); {bonus}\"\n",
    "    body.append(f\"<span style='{style}' title='p={prob:.4e}, rank={rank}, pref={pref}'>{word}</span> \")\n",
    "\n",
    "html = html + body\n",
    "html.append(\"</body></html>\")\n",
    "\n",
    "with open(\"index.html\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89129143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>Example text altered with base prompt</h2>\n",
      "<span style='background-color: #cc1919c8; padding: 2px 5px; border-radius: 3px; outline: 1px solid rgba(0, 0, 0, .2); ' title='p=3.7340e-09, rank=129442, pref= \"'>Football</span> \n",
      "<span style='background-color: #24912464; padding: 2px 5px; border-radius: 3px; outline: 1px solid rgba(0, 0, 0, .2); ' title='p=2.6534e-01, rank=1, pref= is'> is</span> \n",
      "<span style='background-color: #1a8c1a64; padding: 2px 5px; border-radius: 3px; outline: 1px solid rgba(0, 0, 0, .2); ' title='p=3.8352e-01, rank=1, pref= a'> a</span> \n",
      "<span style='background-color: #f9e6e6c8; padding: 2px 5px; border-radius: 3px; outline: 1px solid rgba(0, 0, 0, .2); ' title='p=3.8588e-05, rank=895, pref= game'> fruit</span> \n"
     ]
    }
   ],
   "source": [
    "print(\"<h2>Example text altered with base prompt</h2>\\n\"+\"\\n\".join(body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb953784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html><body style='padding: 20px; font-family: Arial; line-height: 2.0;'>\n",
       "\n",
       "<html>\n",
       "<head>\n",
       "<style>\n",
       "  body { padding: 20px; font-family: Arial, sans-serif; line-height: 2.0; }\n",
       "  h1 { text-align: center; color: #333; font-size: 2.5em; margin-bottom: 20px; }\n",
       "  .word { background-color: #f0f0f0; padding: 2px 5px; border-radius: 3px; outline: 1px solid rgba(0, 0, 0, 0.2); font-weight: bold; }\n",
       "  h2 { color: #333; font-size: 1.8em; margin-bottom: 10px; }\n",
       "  .word { background-color: #f0f0f0; padding: 2px 5px; border-radius: 3px; outline: 1px solid rgba(0, 0, 0, 0.2); font-weight: bold; }\n",
       "</style>\n",
       "</head>\n",
       "<body>\n",
       "    \n",
       "<h1>Text review using LLMs</h1>\n",
       "<h2>Example text with no errors, factual statements</h2>\n",
       "<h2>Example text altered</h2>\n",
       "<span style='background-color: #cc1919c8; padding: 2px 5px; border-radius: 3px; outline: 1px solid rgba(0, 0, 0, .2); ' title='p=3.7340e-09, rank=129442, pref= \"'>Football</span> \n",
       "<span style='background-color: #24912464; padding: 2px 5px; border-radius: 3px; outline: 1px solid rgba(0, 0, 0, .2); ' title='p=2.6534e-01, rank=1, pref= is'> is</span> \n",
       "<span style='background-color: #1a8c1a64; padding: 2px 5px; border-radius: 3px; outline: 1px solid rgba(0, 0, 0, .2); ' title='p=3.8352e-01, rank=1, pref= a'> a</span> \n",
       "<span style='background-color: #f9e6e6c8; padding: 2px 5px; border-radius: 3px; outline: 1px solid rgba(0, 0, 0, .2); ' title='p=3.8588e-05, rank=895, pref= game'> fruit</span> \n",
       "</body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\\n\".join(html)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
